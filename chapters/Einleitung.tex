\documentclass[../Bachelor_LennartKoliwer.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

\textcolor{red}{Fehlen noch quellen natürlich}
Dynamic Mode Decomposition, kurz DMD, ist ein datengetriebener Algorithmus, der ursprünglich im Feld der Strömungsdynamik entwickelt wurde. 
Grundsätzlich dient er der Analyse dynamischer Systeme mittels Sigulärwertzerlegung von großen Datenmatrizen.
DMD kann anhand von Messdaten mehrerer Zeitpunkte die Dynamik eines Systems approximieren. Die Genauigkeit hängt unter anderem davon ab, wie viele Informationen in Form von Messdaten über das System zur Verfügung stehen und wie groß die Messfehler also das Rauschen der Daten ist.
Dabei ist DMD gut im Erkennen von Mustern, wie im Falle der Strömungsdynamik z.B. Verwirbelungen. 
Probleme treten jedoch auf, wenn man DMD außerhalb von Simulationen verwendet und echte Messungen analysieren will. 
Bei Messungen hat man immer ein Rauschen in den Daten, welche das Ergebnis verfälschen.
Dadurch kommt es durch Fehlerfortpflanzung schnell zu größeren Ungenauigkeiten.
Im Laufe der letzten Jahre wurden eine Vielzahl von Varianten der DMD Algorithmen entwickelt, um unterschiedliche Schwächen des Algorithmus auszugleichen.
Das klassische DMD benötigt Messdaten in einheitlichen Zeitabständen.
Für jede Messung werden alle Daten in eine Spalte $x_i$ einer Datenmatrix $X$ eingetragen. 
Zusätzlich legen wir eine zweite Datenmatrix $X'$ so an, dass wir in $X$ die Messungen $(x_1,\dots,x_{k-1})$ und in $X'$ die Messungen $(x_2,\dots,x_k)$ haben.
Die Datenmatrizen sind in der Regel $n\times m$ große Matrizen, wobei sie deutlich mehr Zeilen als Spalten haben, also $n\gg m$ gilt. 
Das Ziel von DMD ist es nun einen linearen Operator $A$ bzw. seine Spektralzerlegung zu finden, welcher das System 
\[
	X' \approx AX
\]
so gut wie möglich beschreibt. Der dafür am besten passende Operator $A$ lässt sich durch 
\[
	A=\underset{A}{\arg\min}\dabs{X'-AX}_F=X'X^\dag
\] 
bestimmen. Dabei sind $\dabs{\cdot}_F$ die Frobeniusnorm und $X^\dag$ die Moore-Penrose-Inverse, welche wir später noch genauer beschreiben werden. 
Die Moore-Penrose-Inverse lässt sich dann mittels Singulärwertzerlegung (SWZ) der Datenmatrix $X$ bestimmen. 
Die SWZ beschreiben wir auch später nochmal genauer, liefern aber für einen kleinen Überblick eine vereinfachte Beschreibung.
Bei der SWZ finden wir für ein $X\in\RR^{n\times m}$ zwei orthogonale Matrizen $U$ und $V$ mit passenden Dimensionen und ${\hat{\Sigma}=\diag(\sigma_1,\dots,\sigma_r)}$ mit $r\leq\min(n,m)$ und $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$ so, dass
\[
	X = U\Sigma V^\top \text{ mit } \Sigma = \begin{pmatrix}
		\hat{\Sigma} & 0 \\
		0            & 0
	\end{pmatrix} \in \mathbb{R}^{n\times m}
\]
gilt.
Bei der Bestimmung von $A$ wird der Rechenaufwand schnell zu groß, weshalb der DMD Algorithmus eine Dimensionsreduktion durchführt. 
Damit kann man eine Spektralzerlegung von $A$ machen, ohne $A$ direkt zu benutzen. 
Durch die oben genannte Form der Datenmatrizen kann man direkt sehen, dass $X$ maximal von Rang$\,m$ sein kann. 
Es gibt also maximal $r\leq m$ Singulärwerte, die nicht Null sind. 
Reduzieren wir also unser $\Sigma$ aus der SWZ auf eine $r\times r$ große Matrix, und entfernen bis auf die ersten $r$ Spalten alle Spalten von $U$ und $V$, erhalten wir mit den reduzierten Matrizen $U_r$, $\Sigma_r$ und $V_r$,
\[
X_r=U_r\Sigma_rV_r^\top=U_r\hat{\Sigma}V_r^\top\,.
\] 
Nach dem Eckart-Young-Mirsky-Theorem \textcolor{red}{Quelle!! Vielleicht doch eher MSE nehmen weil gleich wie im paper} lässt sich durch
\begin{align*}
    \dabs{X_r}_F^2=\sum_{i=1}^{r}\sigma_i^2
\end{align*}
zeigen, dass $X_r$ die beste Rang$\,r$-Approximation bezüglich der Frobeniusnorm ist.
Die Spektralzerlegung von $A$ können wir damit über die viel kleinere Matrix
\[
	\tilde{A}=U_r^\top AU_r
\]
bestimmen.
Diese Methode der Dimensionsreduktion für SWZ, ist als ökonomischer SWZ bekannt.


%---------------------------------------------------------------------------------------


%An diesem Punkt kommt natürlich die Frage auf, ob sich das ganze noch weiter reduzierten lässt, ohne zu viel Information zu verlieren.

Eine weitere Methode, welche im Paper \cite{OHT} von Matan Gavish und David L. Donoho behandelt wird, ist das \glqq singular value hard thresholding\grqq\ (SVHD). Das ist eine Rangreduzierung bis alle Singulärwerte über einem definierten Schwellenwert liegen.
Gavish und Donoho haben herausgefunden, dass innerhalb einer bestimmten Rahmenstruktur 


Plan: (notation aus paper bis auf $\sigma \to \gamma$)
\begin{enumerate}
	\item Rahmenstruktur iid und co
	\item für noise bekannt und $n\times n$ $\hat{\tau}_*=\frac{4}{\sqrt{3}}\gamma\sqrt{n}$
	\item noise unbekannt und $n\times m$ oder andersrum maaaaaaaaaan keine ahnung gerade $\to$ $\hat{\tau}_*(\beta,Y)\coloneqq \lambda_*(\beta)\sqrt{n}\hat{\gamma}(Y)=\frac{\lambda_*(\beta)}{\sqrt{\mu_\beta}}y_{med}$
	\item mit $\omega(\beta)=\frac{\lambda_*(\beta)}{\sqrt{\mu_\beta}}$ dann $\hat{\tau}_*(\beta,Y)=\omega(\beta)y_{med}$
	\item dann $\mu_\beta$ aus MP 
	\item warum MP
	\item was kann MP
	\item ich mach MP
\end{enumerate}







\end{document}

Wie oben erwähnt, tauchen in der Realität beim Messen aber Fehler auf. Das heißt, unsere Singulärwerte sind zu einem Teil echt, aber auch zu einem Teil Messfehler.
Wir definieren die Datenmatrix also durch 
\[
    X \coloneqq \Xtrue+\gamma \Xnoise
\]
einen echten Teil und einem durch $\gamma$ skalierten Fehleranteil.

\textcolor{red}{Überleitung zu MP}
Bei kleinen Matrizen ist das Rauschen kaum ein Problem aber für \glqq große\grqq\ Matrizen kann der Noiseanteil der Singulärwerte sehr groß werden. 

\textcolor{red}{Grobe Richtung} Im Paper \cite{OHT} von Matan Gavish und David L. Donoho erhalten wir durch \textcolor{red}{...}, dass quadratische $n\times n$ Matrizen ab einem Singulärwert von $\tau=\frac{4}{\sqrt{3}}\gamma\sqrt{n}$ abgeschnitten werden. \textcolor{red}{...}



\textcolor{red}{(}Damit können wir die SVD mit $u_i$ und $v_i$ als linke und rechte Singulärvektoren auch durch $X=\sum_{i=1}^{r}\sigma_iu_iv^\top_i$ beschreiben.\textcolor{red}{) hab ich erstmal eingeführt, um einfacher auf die Notation vom Paper zu verweisen (das ist ja aber noch wip)}