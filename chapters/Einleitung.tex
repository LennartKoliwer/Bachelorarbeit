\documentclass[../Bachelor_LennartKoliwer.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}


Dynamic Mode Decomposition, kurz DMD, ist ein datengetriebener Algorithmus, der ursprünglich im Feld der Strömungsdynamik entwickelt wurde. 
Grundsätzlich dient er zur Analyse dynamischer Systeme mittels Sigulärwertzerlegung großer Datenmatrizen.
DMD kann anhand von Messdaten mehrerer Zeitpunkte die Dynamik eines Systems approximieren. Die Genauigkeit hängt unter anderem davon ab, wie viele Informationen in Form von Messdaten man über das System hat und wie groß die Messfehler also das Rauschen der Daten ist.
Dabei ist DMD gut im Erkennen von Mustern, wie im Falle der Strömungsdynamik z.B. Verwirbelungen. 
Probleme treten jedoch auf, wenn man DMD außerhalb von Simulationen befindet und echte Messungen analysieren will. Bei Messungen hat man immer ein Rauschen in den Daten, welche das Ergebnis verfälschen und sich Fehler schnell fortpflanzen.
Im Laufe der letzten Jahre wurden eine Vielzahl von Varianten der DMD Algorithmen entwickelt, um zum Beispiel unterschiedliche Schwächen des Algorithmus auszugleichen.
Das klassische DMD benötigt Messdaten in einheitlichen Zeitabständen.
Für jede Messung werden alle Daten in eine Spalte $x_i$ einer Datenmatrix $X$ eingetragen. 
Zusätzlich legen wir eine zweite Datenmatrix $X'$ so an, dass wir in $X$ die Messungen $(x_1,\dots,x_{k-1})$ und in $X'$ die Messungen $(x_2,\dots,x_k)$ haben.
Die Datenmatrizen sind in der Regel $n\times m$ große Matrizen, wobei sie deutlich mehr Zeilen als Spalten haben, also $n\gg m$ gilt. 
Das Ziel von DMD ist es nun einen linearen Operator $A$ bzw. seine Spektralzerlegung zu finden, welcher das System 
\[
	X' \approx AX
\]
so gut wie möglich beschreibt. Der dafür am besten passende Operator $A$ lässt sich durch 
\[
	A=\underset{A}{\arg\min}\dabs{X'-AX}_F=X'X^\dag
\] 
bestimmen. Dabei sind $\dabs{\cdot}_F$ die Frobeniusnorm und $X^\dag$ die Moore-Penrose-Inverse, welche wir später noch genauer beschreiben werden. 
Die Moore-Penrose-Inverse lässt sich dann mittels Singulärwertzerlegung (SWZ) der Datenmatrix $X$ bestimmen. 
Die SWZ beschreiben wir auch später nochmal genauer, liefern aber für einen kleinen Überblick eine vereinfachte Beschreibung.
Bei der SWZ finden wir für ein $X\in\RR^{n\times m}$ zwei orthogonale Matrizen $U$ und $V$ mit passenden Dimensionen und ${\hat{\Sigma}=\diag(\sigma_1,\dots,\sigma_r)}$ mit $r\leq\min(n,m)$ und $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$ so, dass
\[
	X = U\Sigma V^\top \text{ mit } \Sigma = \begin{pmatrix}
		\hat{\Sigma} & 0 \\
		0            & 0
	\end{pmatrix} \in \mathbb{R}^{n\times m}
\]
gilt.
Bei der Bestimmung von $A$ wird der Rechenaufwand schnell zu groß, weshalb der DMD Algorithmus eine Dimensionsreduktion durchführt. 
Damit kann man eine Spektralzerlegung von $A$ zu machen, ohne $A$ direkt zu benutzen. 
Durch die oben genannte Form der Datenmatrizen kann man direkt sehen, dass $X$ maximal von Rang$\,m$ sein kann. 
Es gibt also maximal $r\leq m$ Singulärwerte, die nicht Null sind. 
Reduzieren wir also unser $\Sigma$ aus der SWZ auf eine $r\times r$ große Matrix, und entfernen bis auf die ersten $r$ Spalten alle Spalten von $U$ und $V$, erhalten wir mit den reduzierten Matrizen $U_r$, $\Sigma_r$ und $V_r$,
\[
X_r=U_r\Sigma_rV_r^\top=U_r\hat{\Sigma}V_r^\top\,.
\] 
Nach dem Eckart-Young-Mirsky-Theorem \textcolor{red}{Quelle!!} lässt sich durch
\begin{align*}
    \dabs{X_r}_F^2=\sum_{i=1}^{r}\sigma_i^2
\end{align*}
zeigen, dass $X_r$ die beste Rang$\,r$-Approximation bezüglich der Frobeniusnorm ist.
Diese Methode der Dimensionsreduktion für SWZ, ist als ökonomischer SWZ bekannt.

%---------------------------------------------------------------------------------------



In der Realität tauchen beim Messen aber Fehler auf. Das heißt, unsere Singulärwerte sind zu einem Teil echt, aber auch zu einem Teil Messfehler.
Wir definieren die Datenmatrix also durch 
\[
    X \coloneqq \Xtrue+\gamma \Xnoise
\]
einen echten Teil und einem durch $\gamma$ skalierten Fehleranteil.


\textcolor{red}{Grobe Richtung} Im Paper \cite{OHT} von Matan Gavish und David L. Donoho erhalten wir durch \textcolor{red}{...}, dass quadratische $n\times n$ Matrizen ab einem Singulärwert von $\tau=\frac{4}{\sqrt{3}}\gamma\sqrt{n}$ abgeschnitten werden. \textcolor{red}{...}


\textcolor{red}{Gavish Donoho weiter ausführen}


\textcolor{red}{Überleitung zu MP}
Bei kleinen Matrizen ist das Rauschen kaum ein Problem aber für \glqq große\grqq\ Matrizen kann der Noiseanteil der Singulärwerte sehr groß werden. 



\end{document}



%\textcolor{red}{(}Damit können wir die SVD mit $u_i$ und $v_i$ als linke und rechte Singulärvektoren auch durch $X=\sum_{i=1}^{r}\sigma_iu_iv^\top_i$ beschreiben.\textcolor{red}{) hab ich erstmal eingeführt, um einfacher auf die Notation vom Paper zu verweisen (das ist ja aber noch wip)}