\documentclass[../Bachelor_LennartKoliwer.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}
\begin{document}

Dynamic Mode Decomposition, kurz DMD, ist ein datengetriebener Algorithmus, der ursprünglich im Feld der Strömungsdynamik entwickelt wurde. 
Er dient dazu, anhand von Messdaten mehrerer Zeitpunkte die Dynamik eines Systems zu approximieren. 
Die Dynamik lässt sich aber nur in kleinem Rahmen prognostizieren.
Bei längeren Prognosen werden Fehler schnell zu groß um noch nutzen aus der Prognose ziehen zu können.
Dabei ist DMD gut im Erkennen von Mustern, wie im Falle der Strömungsdynamik z.B. Strömungen oder Verwirbelungen. 
Ungenau wird DMD aber bei plötzlichen Änderungen des Systems oder wenn die Messdaten fehlerbehaftet sind.
Im Laufe der letzten Jahre wurden eine Vielzahl von Varianten der DMD Algorithmen entwickelt, um z.B. unterschiedliche Schwächen des Algorithmus auszugleichen. \textcolor{red}{viel ungenau erstmal nur orientiereung}

Wir betrachten hier aber nur exact DMD.
Diese Variante hat den Vorteil gegenüber des ursprünglichen DMD, dass die Messzeitpunkte keine einheitlichen Zeitabstände benötigen.
Für jede Messung werden alle Daten der Messpunkte in eine Spalte $x_i$ einer Datenmatrix $X$ eingetragen.
Wenn man zusätzlich parallel nur um einen Zeitpunkt versetzt misst, erhält man zwei Datenmatrizen, die zwei sich überlappende Zeitintervalle abdecken. Genau $X=(x_1,\dots,x_{k-1})$ und $X'=(x_2,\dots,x_k)$.

Das Ziel von DMD ist es nun einen linearen Operator $A$ zu finden, der das System $X' = AX$ so gut wie möglich beschreibt.
Diesen Operator bestimmt man mittels Singulärwertzerlegung unserer Datenmatrix $X$. Die Singulärwertzerlegung (SVD) beschreiben wir später nochmal genauer, liefern aber für einen kleinen Überblick eine vereinfachte Beschreibung.
Bei der SVD finden wir für ein $X\in\RR^{n\times m}$ zwei orthogonale Matrizen $U$ und $V$ mit passenden Dimensionen und ${\hat{\Sigma}=\diag(\sigma_1,\dots,\sigma_r)}$ mit $r\leq\min(n,m)$ und $\sigma_1 \geq \sigma_2 \geq \dots \geq \sigma_r > 0$ so, dass
\[
	X = U\Sigma V^\top \text{ mit } \Sigma = \begin{pmatrix}
		\hat{\Sigma} & 0 \\
		0            & 0
	\end{pmatrix} \in \mathbb{R}^{n\times m}
\]
gilt. \textcolor{red}{(}Damit können wir die SVD mit $u_i$ und $v_i$ als linke und rechte Singulärvektoren auch durch $X=\sum_{i=1}^{r}\sigma_iu_iv^\top_i$ beschreiben.\textcolor{red}{) hab ich erstmal eingeführt, um einfacher auf die Notation vom Paper zu verweisen (das ist ja aber noch wip)}

Um für die folgenden Schritte der DMD den Rechenaufwand zu minimieren, lohnt es sich eine Dimensionsreduktion durchzuführen. Eine Möglichkeit ist als ökonomischer SVD (TSVD) bekannt.
Dabei wird $\Sigma$ auf eine $r\times r$ große Matrix reduziert und die Spalten von $U$ und $V$ abgeschnitten, sodass wir mit den reduzierten Matrizen $U_r$, $\Sigma_r$ und $V_r$, 
\[
X_r=U_r\Sigma_rV_r^\top=U_r\hat{\Sigma}V_r^\top
\] 
erhalten.
Mit der Frobeniusnorm
\begin{align*}
    \dabs{X_r}_F^2=\sum_{i=1}^{r}\sigma_i^2
\end{align*}
lässt sich dann zeigen, dass $X_r$ nach dem Eckart-Young-Mirsky-Theorem die beste Rang$\,r$-Approximation bezüglich der Frobeniusnorm ist.

In der Realität tauchen beim Messen aber Fehler auf. Das heißt, unsere Singulärwerte sind zu einem Teil echt, aber auch zu einem Teil Messfehler.
Wir definieren die Datenmatrix also durch 
\[
    X \coloneqq \Xtrue+\gamma \Xnoise
\]
einen echten Teil und einem durch $\gamma$ skalierten Fehleranteil.


\textcolor{red}{Grobe Richtung} Im Paper \cite{OHT} von Matan Gavish und David L. Donoho erhalten wir durch \textcolor{red}{...}, dass quadratische $n\times n$ Matrizen ab einem Singulärwert von $\tau=\frac{4}{\sqrt{3}}\gamma\sqrt{n}$ abgeschnitten werden. \textcolor{red}{...}


\textcolor{red}{Gavish Donoho weiter ausführen}


\textcolor{red}{Überleitung zu MP}
Bei kleinen Matrizen ist das Rauschen kaum ein Problem aber für \glqq große\grqq\ Matrizen kann der Noiseanteil der Singulärwerte sehr groß werden. 



\end{document}